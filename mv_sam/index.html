<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="description" content="MV-SAM: Multi-view Promptable Segmentation using Pointmap Guidance">
  <meta name="keywords" content="Visual Geometry Model, Pointmap, Multi-view Segmentation, Promptable Segmentation, Computer Vision">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MV-SAM: Multi-view Promptable Segmentation using Pointmap Guidance</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="../static/css/bulma.min.css">
  <link rel="stylesheet" href="../static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="../static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="../static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="../static/js/fontawesome.all.min.js"></script>
  <script src="../static/js/bulma-carousel.min.js"></script>
  <script src="../static/js/index.js"></script>

  <style>
    /* Custom Styles */
    .highlight-section {
      text-align: center;
      /* Reduced top margin to fix empty space issue */
      margin: 10px auto 40px auto; 
      padding: 0 20px;
      max-width: 900px;
    }
    .project-highlight {
      font-size: 1.5rem;
      font-weight: 500;
      color: #222;
      line-height: 1.6;
    }
    .highlight {
      color: #76B900; /* NVIDIA Green */
      font-weight: 800;
    }
    .video-container {
      width: 100%;
      max-width: 960px;
      margin: 0 auto;
      border-radius: 12px;
      overflow: hidden;
      box-shadow: 0 4px 20px rgba(0,0,0,0.3);
      line-height: 0;
    }
    .video-container video {
      width: 100%;
      height: auto;
      display: block;
    }
    /* Reduce Hero Padding */
    .hero-body {
      padding-bottom: 1rem !important; 
    }
  </style>

<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-3467857769900406"
     crossorigin="anonymous"></script>

</head>
<body>

<button class="scroll-to-top" onclick="window.scrollTo({top: 0, behavior: 'smooth'});" title="Scroll to top" style="position:fixed; bottom:20px; right:20px; z-index:99; display:none;">
  <i class="fas fa-chevron-up"></i>
</button>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          
          <h1 class="title is-1 publication-title">
            <span style="color:#76B900;">MV-SAM</span>: Multi-view Promptable Segmentation using Pointmap Guidance
          </h1>
          
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://jeongyw12382.github.io" target="_blank">Yoonwoo Jeong</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="https://sunset1995.github.io" target="_blank">Cheng Sun</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://research.nvidia.com/person/frank-wang" target="_blank">Yu-Chiang Frank Wang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://cvlab.postech.ac.kr/~mcho" target="_blank">Minsu Cho</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://jaesung-choe.github.io" target="_blank">Jaesung Choe</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>NVIDIA,</span>
            <span class="author-block"><sup>2</sup>POSTECH,</span>
            <br>
            <span class="author-block"><strong>arXiv 2026</strong></span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="#" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fas fa-file-pdf"></i></span>
                  <span>Paper (TBU)</span>
                </a>
              </span>

              <span class="link-block">
                <a href="#" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fab fa-github"></i></span>
                  <span>Code (TBU)</span>
                </a>
              </span>
            </div>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</section>

<section class="highlight-section">
  <h2 class="project-highlight">
    <strong><span class="highlight">MV-SAM</span></strong> extends Segment Anything Model (SAM) into multi-view images without using annotated 3D or video datasets. <br>
    ðŸ”¥ Ours outperform SAMv3 as well.
  </h2>
</section>
  
<section class="section" style="background-color: #f5f5f5;">
  <div class="container">
    <h2 class="title is-3 has-text-centered">Video Teaser</h2>
    
    <div class="video-container">
      <video src="videos/banner.mp4" controls autoplay loop muted playsinline></video>
    </div>

  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Promptable segmentation has emerged as a powerful paradigm in computer vision, enabling users to guide models in parsing complex scenes with prompts such as clicks, boxes, or textual cues. Recent advances, exemplified by the Segment Anything Model (SAM), have extended this paradigm to videos and multi-view images. However, the lack of 3D awareness often leads to inconsistent results, necessitating costly per-scene optimization to enforce 3D consistency. In this work, we introduce <strong>MV-SAM</strong>, a framework for multi-view segmentation that achieves 3D consistency using pointmaps--3D points reconstructed from unposed images by recent visual geometry models. Leveraging the pixelâ€“point one-to-one correspondence of pointmaps, MV-SAM lifts images and prompts into 3D space, eliminating the need for explicit 3D networks or annotated 3D data. Specifically, MV-SAM extends SAM by lifting image embeddings from its pretrained encoder into 3D point embeddings, which are decoded by a transformer using cross-attention with 3D prompt embeddings. This design aligns 2D interactions with 3D geometry, enabling the model to implicitly learn consistent masks across views through 3D positional embeddings. Trained on the SA-1B dataset, our method generalizes well across domains, outperforming SAM2-Video and achieving comparable performance with per-scene optimization baselines on NVOS, SPIn-NeRF, ScanNet++, uCo3D, and DL3DV benchmarks. Code will be released.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 has-text-centered">Method & Results</h2>
      <div id="results-carousel" class="carousel results-carousel">
        
        <div class="item">
          <img src="images/main.png" alt="Method 2" style="width:100%; object-fit:contain; max-height:600px; display:block; margin: 0 auto;"/>
          <h2 class="subtitle has-text-centered" style="margin-top:15px;">
            We introduce <span class="highlight">MV-SAM</span> where users can provide prompts in multi-view images for user-interactive segmentation.
          </h2>
        </div>

        <div class="item">
          <img src="images/method.png" alt="Comparison" style="width:100%; object-fit:contain; max-height:600px; display:block; margin: 0 auto;"/>
          <h2 class="subtitle has-text-centered" style="margin-top:15px;">
            Unlike mask propagation of SAM2-Video, which lacks 3D understanding, our <span class="highlight">MV-SAM</span> propagates all prompts in a consistent manner using pointmap.
          </h2>
        </div>

        <div class="item">
          <img src="images/data.png" alt="Method 1" style="width:100%; object-fit:contain; max-height:600px; display:block; margin: 0 auto;"/>
          <h2 class="subtitle has-text-centered" style="margin-top:15px;">
            By removing the reliance on 3D explicit networks or 3D inductive bias, we first train a 3D foundation model on a <span class="highlight">2D large-scale dataset</span>, SA-1B.
          </h2>
        </div>

        <div class="item">
          <img src="images/qual.png" alt="Results" style="width:100%; object-fit:contain; max-height:600px; display:block; margin: 0 auto;"/>
          <h2 class="subtitle has-text-centered" style="margin-top:15px;">
            Thereby, our <span class="highlight">MV-SAM</span> enables 3D promptable segmentation in various domains, demonstrating strong generalizability.
          </h2>
        </div>

      </div>
    </div>
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 has-text-centered">Video results:<br>(left) SAMv2, (mid) Ours, (right) Ground truth</h2>
      <div id="results-carousel" class="carousel results-carousel">
        
        <div class="item item-video1">
          <!-- TODO: Add poster image for better preview -->
          <video poster="" id="video1" controls muted loop height="100%" preload="metadata">
            <!-- Your video file here -->
            <source src="videos/sample1.mp4" type="video/mp4">
          </video>
        </div>

        <div class="item item-video2">
          <!-- TODO: Add poster image for better preview -->
          <video poster="" id="video2" controls muted loop height="100%" preload="metadata">
            <!-- Your video file here -->
            <source src="videos/sample2.mp4" type="video/mp4">
          </video>
        </div>

        <div class="item item-video3">
          <!-- TODO: Add poster image for better preview -->
          <video poster="" id="video3" controls muted loop height="100%" preload="metadata">
            <!-- Your video file here -->
            <source src="videos/sample3.mp4" type="video/mp4">
          </video>
        </div>

      </div>
    </div>
  </div>
</section>

<section class="section" style="background-color: #f5f5f5;">
  <div class="container">
    <h2 class="title is-3 has-text-centered">Video summary</h2>
    
    <div class="video-container">
      <video src="videos/video.mp4" controls autoplay loop muted playsinline></video>
    </div>

  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{jeong2025mv,
  title={MV-SAM: Multi-view Promptable Segmentation using Pointmap Guidance},
  author={Jeong, Yoonwoo and Sun, Cheng and Wang, Yu-Chiang Frank and Cho, Minsu and Choe, Jaesung},
  journal={arXiv},
  year={2026},
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <p>
        Website template from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
      </p>
    </div>
  </div>
</footer>

<script>
document.addEventListener('DOMContentLoaded', () => {
  const videos = document.querySelectorAll('video');
  videos.forEach(video => {
    video.addEventListener('loadeddata', () => {
        // Optional: Force jump to 0.1s to ensure a frame is rendered for poster
        video.currentTime = 0.1;
    });
  });
});
</script>

</body>
</html>
