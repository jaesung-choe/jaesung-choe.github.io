<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Jaesung Choe</title>
  <meta name="author" content="Jaesung Choe">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <!-- Introduction -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Jaesung Choe</name>
                  </p>
                  <p> Hi, I am a research scientist at <a href="https://research.nvidia.com/labs/twn/" style="color: #76B900">NVIDIA Research Taiwan</a>. My main research interest is 3D vision language models from 3D pointclouds, 3D Gaussians, and 3D humans. </p>
                  <p> I received my PhD from <a href="https://www.kaist.ac.kr/en/">KAIST</a> under the supervision of <a href="https://rcv.kaist.ac.kr/" style="color: #1a428a">In So Kweon</a>. I finished my research internship at <a href="https://www.nvidia.com/en-us/research/" style="color: #76B900">NVIDIA Research</a> with <a href="https://chrischoy.github.io/">Christopher Choy</a>, <a href="http://tensorlab.cms.caltech.edu/users/anima/">Anima Anandkumar</a>, and at <a href="https://about.meta.com/realitylabs/" style="color: #3d03fc">Meta</a> with <a href="https://shuochsu.github.io/">Shuochen Su</a>. </p>
                  <p style="text-align:center">
                    <a href="https://drive.google.com/file/d/1rFHwf0bPE34dMwDoAEDkbrDHB0GGXa2K/view?usp=sharing">R√©sum√©</a> &nbsp/&nbsp jchoe _at_ nvidia _dot_ com &nbsp/&nbsp <a href="https://scholar.google.com/citations?user=dQ7NO0IAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp <a href="https://www.linkedin.com/in/jaesung-choe-531611267/">LinkedIn</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/jaesungchoe-profile-3.jpeg"><img style="width:80%;max-width:80%" alt="profile photo" src="images/jaesungchoe-profile-3.jpeg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>
          <!-- News -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>News</heading>
                  <p>
                    If you're interested in a 2025 internship, please (1) send me an email, and submit your application at (2) <a href="https://nvidia.wd5.myworkdayjobs.com/en-US/NVIDIAExternalCareerSite/job/Taiwan-Taipei/Research-Intern--Deep-Learning-and-Artificial-Intelligence---2025_JR1989153?q=Research%20intern&locationHierarchy1=2fcb99c455831013ea52ed162d4932c0">NVIDIA career</a>.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <!-- Research overview -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Research</heading>
                  <p>
                    I'm interested in computer vision for 3D scene understanding, such as (1) 3D Visual Language Model, (2) 3D Scene Reconstruction and Neural Rendering and (3) 3D Vehicle/Object Perception. Some papers are <span class="highlight">highlighted</span>.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <!-- Research (publications) -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <!-- Works listed here -->
              <!-- Dr.Splat -->
              <tr onmouseout="dr_splat_stop()" onmouseover="dr_splat_start()">
                <td style="padding:16px;width:20%;vertical-align:top;">
                  <!-- IMAGES -->
                  <div class="one">
                    <div class="two" id='dr_splat_image'><video  width=120% muted autoplay loop>
                    <source src="images/dr_splat_under.png" type="video/mp4">
                    </video></div>
                    <img src='images/dr_splat_under.png' width=120%>
                  </div>
                  <script type="text/javascript">
                    function dr_splat_start() {
                      document.getElementById('dr_splat_image').style.opacity = "1";
                    }
                    function dr_splat_stop() {
                      document.getElementById('dr_splat_image').style.opacity = "0";
                    }
                    dr_splat_stop()
                  </script>
                </td>
                <td style="padding:16px;width:80%;vertical-align:top;">
                  <!-- TITLE -->
                  <a href="">
                    <papertitle>Dr. Splat: Directly Referring 3D Gaussian Splatting via Direct Language Embedding Registration</papertitle>
                  </a> <br>
                  <!-- AUTHORS -->
                  Kim Jun-Seong, GeonU Kim, Kim Yu-Ji, Yu-Chiang Frank Wang, <b>Jaesung Choe*</b>, Tae-Hyun Oh*<br>
                  <!-- CONFERENCE -->
                  <em>arXiv,</em> 2025 <br>
                  <!-- LINKS -->
                  [<a href="">Project</a>] [<a href="https://arxiv.org/pdf/2502.16652">Paper</a>] <br>
                  <!-- SUMMARY -->
                  <p>
                    Distil language knowledges into 3D Gaussian Splatting.
                  </p>
                </td>
              </tr>
              <!-- Works listed here -->
              <!-- Mosaic3D -->
              <tr onmouseout="mosaic_stop()" onmouseover="mosaic_start()" bgcolor="#defaaf">
                <td style="padding:16px;width:20%;vertical-align:top;">
                  <!-- IMAGES -->
                  <div class="one">
                    <div class="two" id='mosaic_image'><video  width=120% muted autoplay loop>
                    <source src="images/mosaic_under.png" type="video/mp4">
                    </video></div>
                    <img src='images/mosaic_under.png' width=120%>
                  </div>
                  <script type="text/javascript">
                    function mosaic_start() {
                      document.getElementById('mosaic_image').style.opacity = "1";
                    }
                    function mosaic_stop() {
                      document.getElementById('mosaic_image').style.opacity = "0";
                    }
                    mosaic_stop()
                  </script>
                </td>
                <td style="padding:16px;width:80%;vertical-align:top;">
                  <!-- TITLE -->
                  <a href="">
                    <papertitle>Mosaic3D: Foundation Dataset and Model for Open-Vocabulary 3D Segmentation</papertitle>
                  </a> <br>
                  <!-- AUTHORS -->
                  Junha Lee*, Chunghyun Park*, <b>Jaesung Choe</b>, Yu-Chiang Frank Wang, Jan Kautz, Minsu Cho, Christopher Choy<br>
                  <!-- CONFERENCE -->
                  <em>arXiv,</em> 2025 <br>
                  <!-- LINKS -->
                  [<a href="https://nvlabs.github.io/Mosaic3D/">Project</a>] [<a href="https://arxiv.org/pdf/2502.02548">Paper</a>] <br>
                  <!-- SUMMARY -->
                  <p>
                    Introduce a data generation pipeline that leverages 2D foundation models to synthesize 3D mask-text paired data.
                  </p>
                </td>
              </tr>
              <!-- Works listed here -->
              <!-- VoxRas -->
              <tr onmouseout="voxras_stop()" onmouseover="voxras_start()" bgcolor="#defaaf">
                <td style="padding:16px;width:20%;vertical-align:top;">
                  <!-- IMAGES -->
                  <div class="one">
                    <div class="two" id='voxras_image'><video  width=120% muted autoplay loop>
                    <source src="images/voxras_under.png" type="video/mp4">
                    </video></div>
                    <img src='images/voxras_under.png' width=120%>
                  </div>
                  <script type="text/javascript">
                    function voxras_start() {
                      document.getElementById('voxras_image').style.opacity = "1";
                    }
                    function voxras_stop() {
                      document.getElementById('voxras_image').style.opacity = "0";
                    }
                    voxras_stop()
                  </script>
                </td>
                <td style="padding:16px;width:80%;vertical-align:top;">
                  <!-- TITLE -->
                  <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Choe_Spacetime_Surface_Regularization_for_Neural_Dynamic_Scene_Reconstruction_ICCV_2023_paper.pdf">
                    <papertitle>Sparse Voxels Rasterization: Real-time High-fidelity Radiance Field Rendering</papertitle>
                  </a> <br>
                  <!-- AUTHORS -->
                  Cheng Sun, <b>Jaesung Choe</b>, Charles Loop, Wei-Chiu Ma, Yu-Chiang Frank Wang <br>
                  <!-- CONFERENCE -->
                  <em>arXiv,</em> 2024 <br>
                  <!-- LINKS -->
                  [<a href="https://arxiv.org/abs/2412.04459">Paper</a>] <br>
                  <!-- SUMMARY -->
                  <p>
                    Extend to use the rasterization technique into the sparse voxel representation for the image rendering task.
                  </p>
                </td>
              </tr>
              <!-- Works listed here -->
              <!-- 4DRegSDF -->
              <tr onmouseout="regsdf_stop()" onmouseover="regsdf_start()">
                <td style="padding:20px;width:30%;vertical-align:top;">
                  <!-- IMAGES -->
                  <div class="one">
                    <video width=120% height=100% muted autoplay loop>
                      <source src="images/regsdf.mp4" type="video/mp4">
                    </video>
                  </div>
                  <!-- SCRIPT FOR HOVERING -->
                  <script type="text/javascript">
                    function regsdf_start() {
                      document.getElementById('regsdf_splash').style.opacity = "0";
                      document.getElementById('regsdf_under').style.opacity = "1";
                    }
                    function regsdf_stop() {
                      document.getElementById('regsdf_splash').style.opacity = "1";
                      document.getElementById('regsdf_under').style.opacity = "0";
                    }
                    regsdf_stop()
                  </script>
                </td>
                <td style="padding:20px;width:70%;vertical-align:top;">
                  <!-- TITLE -->
                  <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Choe_Spacetime_Surface_Regularization_for_Neural_Dynamic_Scene_Reconstruction_ICCV_2023_paper.pdf">
                    <papertitle>Spacetime Surface Regularization for Neural Dynamic Scene Reconstruction</papertitle>
                  </a> <br>
                  <!-- AUTHORS -->
                  <b>Jaesung Choe</b>, Christopher Choy, Jaesik Park, In So Kweon, Anima Anandkumar <br>
                  <!-- CONFERENCE -->
                  <em>ICCV,</em> 2023 <br>
                  <!-- LINKS -->
                  [<a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Choe_Spacetime_Surface_Regularization_for_Neural_Dynamic_Scene_Reconstruction_ICCV_2023_paper.pdf">Paper</a>] <br>
                  <p></p>
                  <!-- SUMMARY --> Apply spacetime surface regularization technique for 4D surface reconstruction and dynamic scene rendering.
                </td>
              </tr>
              <!-- Works listed here -->
              <!-- PointMIxer -->
              <tr onmouseout="pointmixer_stop()" onmouseover="pointmixer_start()">
                <td style="padding:16px;width:20%;vertical-align:top;">
                  <!-- IMAGES -->
                  <div class="one">
                    <div class="two" id='pointmixer_under'>
                      <img src="images/pointmixer_under.jpeg" width=120% />
                    </div>
                    <img src="images/pointmixer_under.jpeg" width=120% />
                  </div>
                  <!-- SCRIPT FOR HOVERING -->
                  <script type="text/javascript">
                    function pointmixer_start() {
                      document.getElementById('pointmixer_splash').style.opacity = "0";
                      document.getElementById('pointmixer_under').style.opacity = "1";
                    }

                    function pointmixer_stop() {
                      document.getElementById('pointmixer_splash').style.opacity = "1";
                      document.getElementById('pointmixer_under').style.opacity = "0";
                    }
                    pointmixer_stop()
                  </script>
                </td>
                <td style="padding:16px;width:80%;vertical-align:top;">
                  <!-- TITLE -->
                  <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136870611.pdf">
                    <papertitle>PointMixer: MLP-Mixer for Point Cloud Understanding </papertitle>
                  </a>
                  <br>
                  <!-- AUTHORS -->
                  <b>Jaesung Choe*</b>, Chunghyun Park*, Francois Rameau, Jaesik Park, In So Kweon <br>
                  <!-- *Equal contribution <br> -->
                  <!-- CONFERENCE -->
                  <em>ECCV,</em> 2022 <br>
                  <!-- LINKS -->
                  [<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136870611.pdf">Paper</a>] [<a href="https://github.com/LifeBeyondExpectations/ECCV22-PointMixer">Code</a>] [<a href="https://www.dropbox.com/s/im5r5jc2kdej2hb/ECCV22_Poster.pdf?dl=0">Poster</a>] [<a href="https://www.youtube.com/watch?v=96jY3DEjxxo&feature=youtu.be&themeRefresh=1">Video</a>]
                  <p></p>
                  <!-- SUMMARY --> Design a new MLP-only architecture for 3D points.
                  Introduced by <a href="https://x.com/_akhaliq/status/1462966856428441604">AK</a>.
                </td>
              </tr>
              <!-- Works listed here -->
              <!-- PointRecon -->
              <tr onmouseout="pointrecon_stop()" onmouseover="pointrecon_start()">
                <td style="padding:16px;width:20%;vertical-align:top;">
                  <!-- IMAGES -->
                  <div class="one">
                    <div class="two" id='pointrecon_under'>
                      <img src="images/pointrecon_under.png" width=120% />
                    </div>
                    <img src="images/pointrecon_under.png" width=120% />
                  </div>
                  <!-- SCRIPT FOR HOVERING -->
                  <script type="text/javascript">
                    function pointrecon_start() {
                      document.getElementById('pointrecon_splash').style.opacity = "0";
                      document.getElementById('pointrecon_under').style.opacity = "1";
                    }

                    function pointrecon_stop() {
                      document.getElementById('pointrecon_splash').style.opacity = "1";
                      document.getElementById('pointrecon_under').style.opacity = "0";
                    }
                    pointrecon_stop()
                  </script>
                </td>
                <td style="padding:16px;width:80%;vertical-align:top;">
                  <!-- TITLE -->
                  <a href="https://openreview.net/pdf?id=mKDtUtxIGJ">
                    <papertitle>Deep Point Cloud Reconstruction </papertitle>
                  </a> <br>
                  <!-- AUTHORS -->
                  <b>Jaesung Choe*</b>, ByeongIn Joung*, Francois Rameau, Jaesik Park, In So Kweon <br>
                  <!-- CONFERENCE -->
                  <em>ICLR,</em> 2022 <br>
                  <!-- LINKS -->
                  [<a href="https://openreview.net/pdf?id=mKDtUtxIGJ">Paper</a>] <br>
                  <p></p>
                  <!-- SUMMARY --> Perform the point cloud upsampling and denoising tasks simultaneously. Demonstrate good generalization performance.
                </td>
              </tr>
              <!-- Works listed here -->
              <!-- depthfusion -->
              <tr onmouseout="depthfusion_stop()" onmouseover="depthfusion_start()">
                <td style="padding:16px;width:20%;vertical-align:top;">
                  <!-- IMAGES -->
                  <div class="one">
                    <div class="two" id="depthfusion_under" style="opacity: 0;">
                      <img src="images/depthfusion_under.png" width="120%">
                    </div>
                    <img src="images/depthfusion_under.png" width="120%">
                  </div>
                  <!-- SCRIPT FOR HOVERING -->
                  <script type="text/javascript">
                    function depthfusion_start() {
                      document.getElementById('depthfusion_splash').style.opacity = "0";
                      document.getElementById('depthfusion_under').style.opacity = "1";
                    }
                    function depthfusion_stop() {
                      document.getElementById('depthfusion_splash').style.opacity = "1";
                      document.getElementById('depthfusion_under').style.opacity = "0";
                    }
                    depthfusion_stop()
                  </script>
                </td>
                <td style="padding:16px;width:80%;vertical-align:top;">
                  <!-- TITLE -->
                  <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Choe_VolumeFusion_Deep_Depth_Fusion_for_3D_Scene_Reconstruction_ICCV_2021_paper.html">
                    <papertitle>VolumeFusion: Deep Depth Fusion for 3D Scene Reconstruction</papertitle>
                  </a> <br>
                  <!-- AUTHORS -->
                  <b>Jaesung Choe</b>, Sunghoon Im, Francois Rameau, Minjun Kang, In So Kweon <br>
                  <!-- CONFERENCE -->
                  <em>ICCV,</em> 2021 <br>
                  <!-- LINKS -->
                  [<a href="https://openaccess.thecvf.com/content/ICCV2021/html/Choe_VolumeFusion_Deep_Depth_Fusion_for_3D_Scene_Reconstruction_ICCV_2021_paper.html">Paper</a>] [<a href="https://youtu.be/NMB9_dch4BI">Video</a>] [<a href="https://www.dropbox.com/s/x61mzk8go18rcmm/Video_VolumeFusion.pdf?dl=0">Slide</a>]
                  <p></p>
                  <!-- SUMMARY --> Propose deep learning based depth fusion algorithm for indoor scene reconstruction.
                </td>
              </tr>
              <!-- Works listed here -->
              <!-- stereolidar -->
              <tr onmouseout="stereolidar_stop()" onmouseover="stereolidar_start()">
                <td style="padding:16px;width:20%;vertical-align:top;">
                  <!-- IMAGES -->
                  <div class="one">
                    <div class="two" id="stereolidar_under" style="opacity: 0;">
                      <img src="images/stereolidar_under.png" width="120%">
                    </div>
                    <img src="images/stereolidar_under.png" width="120%">
                  </div>
                  <!-- SCRIPT FOR HOVERING -->
                  <script type="text/javascript">
                    function stereolidar_start() {
                      document.getElementById('stereolidar_splash').style.opacity = "0";
                      document.getElementById('stereolidar_under').style.opacity = "1";
                    }
                    function stereolidar_stop() {
                      document.getElementById('stereolidar_splash').style.opacity = "1";
                      document.getElementById('stereolidar_under').style.opacity = "0";
                    }
                    stereolidar_stop()
                  </script>
                </td>
                <td style="padding:16px;width:80%;vertical-align:top;">
                  <!-- TITLE -->
                  <a href="https://ieeexplore.ieee.org/document/9385917">
                    <papertitle>Volumetric Propagation Network: Stereo-LiDAR Fusion for Long-Range Depth Estimation</papertitle>
                  </a> <br>
                  <!-- AUTHORS -->
                  <b>Jaesung Choe</b>, Kyungdon Joo, Tooba Imtiaz, In So Kweon <br>
                  <!-- CONFERENCE -->
                  <em>RA-L,</em> 2021 <br>
                  <!-- LINKS -->
                  <br>
                  [<a href="https://ieeexplore.ieee.org/document/9385917">Paper (RA-L)</a>] [<a href="https://arxiv.org/abs/2103.12964">Paper (ICRA)</a>] [<a href="https://youtu.be/2A5NwNJwb18">Video</a>]
                  <p></p>
                  <!-- SUMMARY --> Estimation dense depth maps by fusing LiDAR points and stereo images.
                </td>
              </tr>
            </tbody>
          </table>
          <!-- Mentorship -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Mentorship</heading>
                  <p>
                    <a href="https://scholar.google.com/citations?user=HQ1PMggAAAAJ&hl=en">Yoonwoo Jeong</a> (research intern - 2024) <br>
                    <a href="https://chrockey.github.io/">Chunghyun Park</a> (research intern - 2023) <br>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <!-- Academic collaboration -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Academic collaboration</heading>
                  <p>
                    <a href="https://ug-kim.github.io/">Kim Yu-Ji</a> (POSTECH - 2025) <br>
                    <a href="https://jiyeon-k1m.github.io/">Kim Ji-Yeon</a> (POSTECH - 2024) <br>
                    <a href="https://kim-junseong.github.io/">Kim Jun-Seong</a> (POSTECH - 2024) <br>
                    <a href="https://sites.google.com/view/minjun-kang">Minjun Kang</a> (KAIST - 2023) <br>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <!-- Talks -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Talks</heading>
                  <p>
                    Pusan Univ., Sep 2024 <br> Hangyang Univ., Sep 2024 <br> DGIST, April 2024 <br> Kakao Brain, May 2022 <br>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <!-- Source from Jon Barron -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:left;font-size:small;">
                    Source code for this page was taken from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron's website</a>.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
    </tbody>
  </table>
</body>

</html>
